{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv(\"/home/ander/Documentos/Universidad/ML/proyecto/train_processed.csv\")\n",
    "\n",
    "def one_hot_encode_categorical(df, column):\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_column = encoder.fit_transform(df[[column]])    \n",
    "    encoded_df = pd.DataFrame(encoded_column, columns=encoder.get_feature_names_out([column]))\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df, encoder\n",
    "\n",
    "categorical_columns = ['Geography']\n",
    "df = df.copy()  \n",
    "\n",
    "encoders = {}\n",
    "for column in categorical_columns:\n",
    "    df, enconder = one_hot_encode_categorical(df, column)\n",
    "    encoders[column] = enconder\n",
    "    \n",
    "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X = df.drop(columns=[\"Exited\"])\n",
    "y = df[\"Exited\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43d695",
   "metadata": {},
   "source": [
    "OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import create_study\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective_rf(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    X_train_use, X_test_use = X_train, X_test\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_gb(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0)\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**params, random_state=42)\n",
    "    X_train_use, X_test_use = X_train, X_test\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "    model = XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    X_train_use, X_test_use = X_train, X_test\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 2)\n",
    "    }\n",
    "    model = LGBMClassifier(**params, random_state=42)\n",
    "    X_train_use, X_test_use = X_train, X_test\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_lr(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.01, 10.0),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
    "    }\n",
    "    model = LogisticRegression(**params, random_state=42, max_iter=2000)\n",
    "    X_train_use, X_test_use = X_train_scaled, X_test_scaled\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_et(trial):\n",
    "    sampler_name = trial.suggest_categorical('sampler', ['None', 'SMOTE', 'ADASYN'])\n",
    "    if sampler_name == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler_name == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    }\n",
    "    model = ExtraTreesClassifier(**params, random_state=42)\n",
    "    X_train_use, X_test_use = X_train, X_test\n",
    "    if sampler is not None:\n",
    "        X_res, y_res = sampler.fit_resample(X_train_use, y_train)\n",
    "    else:\n",
    "        X_res, y_res = X_train_use, y_train\n",
    "    model.fit(X_res, y_res)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective_easyensemble(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300)\n",
    "    }\n",
    "    model = EasyEnsembleClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        random_state=42, n_jobs=8\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_rusboost(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        estimator=DecisionTreeClassifier(max_depth=params['max_depth'], random_state=42),\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_balancedbagging(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = BalancedBaggingClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_samples=params['max_samples'],\n",
    "        max_features=params['max_features'],\n",
    "        estimator=DecisionTreeClassifier(max_depth=params['max_depth'], random_state=42),\n",
    "        random_state=42, n_jobs=8\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score\n",
    "\n",
    "def objective_balancedrf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    }\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        max_features=params['max_features'],\n",
    "        random_state=42, n_jobs=8\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna para EasyEnsembleClassifier\n",
    "study_easyensemble = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_easyensemble.optimize(objective_easyensemble, n_trials=250)\n",
    "print(\"EasyEnsembleClassifier best params:\", study_easyensemble.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna para RUSBoostClassifier\n",
    "study_rusboost = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_rusboost.optimize(objective_rusboost, n_trials=250)\n",
    "print(\"RUSBoostClassifier best params:\", study_rusboost.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7de3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna para BalancedBaggingClassifier\n",
    "study_balancedbagging = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_balancedbagging.optimize(objective_balancedbagging, n_trials=250)\n",
    "print(\"BalancedBaggingClassifier best params:\", study_balancedbagging.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164acb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optuna para BalancedRandomForestClassifier\n",
    "study_balancedrf = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_balancedrf.optimize(objective_balancedrf, n_trials=250)\n",
    "print(\"BalancedRandomForestClassifier best params:\", study_balancedrf.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecuta cada estudio por separado\n",
    "study_rf = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_rf.optimize(objective_rf, n_trials=100)\n",
    "print(\"RandomForest:\", study_rf.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_gb = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_gb.optimize(objective_gb, n_trials=100)\n",
    "print(\"GradientBoosting:\", study_gb.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b699ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "study_xgb = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_xgb.optimize(objective_xgb, n_trials=100)\n",
    "print(\"XGBoost:\", study_xgb.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c879376",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lgb = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_lgb.optimize(objective_lgb, n_trials=500)\n",
    "print(\"LightGBM:\", study_lgb.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lr = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_lr.optimize(objective_lr, n_trials=100)\n",
    "print(\"LogisticRegression:\", study_lr.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e472e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_et = create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_et.optimize(objective_et, n_trials=100)\n",
    "print(\"ExtraTrees:\", study_et.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ad7f3",
   "metadata": {},
   "source": [
    "(IMBLEARN_EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Entrena el modelo EasyEnsembleClassifier con los mejores parámetros\n",
    "best_eec = EasyEnsembleClassifier(\n",
    "    n_estimators=59,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_eec.fit(X_train, y_train)\n",
    "y_pred = best_eec.predict(X_test)\n",
    "y_pred_proba = best_eec.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluar el modelo\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2f21a",
   "metadata": {},
   "source": [
    "(IMBLEARN_RUSB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Entrena el modelo RUSBoostClassifier. \n",
    "# Los parámetros del estimador base (DecisionTreeClassifier) se pasan con '__'.\n",
    "best_rus = RUSBoostClassifier(\n",
    "    n_estimators=295,\n",
    "    learning_rate=0.03169408226050428,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_rus.fit(X_train, y_train)\n",
    "y_pred = best_rus.predict(X_test)\n",
    "y_pred_proba = best_rus.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluar el modelo\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101ec78",
   "metadata": {},
   "source": [
    "(IMBLEARN_BBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccf8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Nota: El parámetro max_depth pertenece al estimador base (DecisionTreeClassifier).\n",
    "# Se debe instanciar y pasar al modelo BalancedBaggingClassifier.\n",
    "\n",
    "# Entrena el modelo BalancedBaggingClassifier con los mejores parámetros\n",
    "best_bbc = BalancedBaggingClassifier(\n",
    "\n",
    "    n_estimators=293,\n",
    "    max_samples=0.7683980382109316,\n",
    "    max_features=0.8868546463344018,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_bbc.fit(X_train, y_train)\n",
    "y_pred = best_bbc.predict(X_test)\n",
    "y_pred_proba = best_bbc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluar el modelo\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39924715",
   "metadata": {},
   "source": [
    "(IMBLEARN_BRFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c955f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Entrena el modelo BalancedRandomForestClassifier con los mejores parámetros\n",
    "best_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=181,\n",
    "    max_depth=7,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_brf.fit(X_train, y_train)\n",
    "y_pred = best_brf.predict(X_test)\n",
    "y_pred_proba = best_brf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluar el modelo\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0ead4",
   "metadata": {},
   "source": [
    "(OPTUNA_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=136,\n",
    "    max_depth=10,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='log2',\n",
    "    bootstrap=False,\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar sobre el conjunto de prueba\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ed39b",
   "metadata": {},
   "source": [
    "(OPTUNA_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b16436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Crear el pipeline que integra SMOTE y el clasificador\n",
    "pipeline_gb = Pipeline([\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=261,\n",
    "        learning_rate=0.03643803195448914,\n",
    "        max_depth=5,\n",
    "        min_samples_split=8,\n",
    "        min_samples_leaf=6,\n",
    "        subsample=0.7970512036903703,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Entrenar el pipeline completo\n",
    "pipeline_gb.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluar sobre el conjunto de prueba original (el pipeline maneja todo internamente)\n",
    "y_pred = pipeline_gb.predict(X_test)\n",
    "y_pred_proba = pipeline_gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69831dc7",
   "metadata": {},
   "source": [
    "(OPTUNA_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "best_xgb = XGBClassifier(\n",
    "    n_estimators=108,\n",
    "    learning_rate=0.09830372052041464,\n",
    "    max_depth=10,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.7373717489359468,\n",
    "    colsample_bytree=0.7679078375986028,\n",
    "    gamma=4.519428792014974,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar sobre el conjunto de prueba\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09693b94",
   "metadata": {},
   "source": [
    "(OPTUNA_LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8043a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# 1. Crear el pipeline que integra ADASYN y el clasificador\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('sampler', ADASYN(random_state=42)),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=157,\n",
    "        learning_rate=0.05320213755207298,\n",
    "        max_depth=6,\n",
    "        min_child_samples=17,\n",
    "        subsample=0.9669128821218812,\n",
    "        colsample_bytree=0.7484949223442707,\n",
    "        reg_alpha=0.6488953630452698,\n",
    "        reg_lambda=0.8732792139771435,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Entrenar el pipeline completo\n",
    "pipeline_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluar sobre el conjunto de prueba original\n",
    "y_pred = pipeline_lgbm.predict(X_test)\n",
    "y_pred_proba = pipeline_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327a9b7",
   "metadata": {},
   "source": [
    "(OPTUNA_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# 1. Crear el pipeline que integra ADASYN y el clasificador\n",
    "pipeline_lr = Pipeline([\n",
    "    ('sampler', ADASYN(random_state=42)),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=9.913728258413286,\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight=None,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Entrenar el pipeline completo\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluar sobre el conjunto de prueba original\n",
    "y_pred = pipeline_lr.predict(X_test)\n",
    "y_pred_proba = pipeline_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a694bd",
   "metadata": {},
   "source": [
    "(OPTUNA_ExTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce962854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "best_et = ExtraTreesClassifier(\n",
    "    n_estimators=375,\n",
    "    max_depth=14,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='log2',\n",
    "    random_state=42\n",
    ")\n",
    "best_et.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar sobre el conjunto de prueba\n",
    "y_pred = best_et.predict(X_test)\n",
    "y_pred_proba = best_et.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "gmean = (sensibilidad * especificidad) ** 0.5\n",
    "\n",
    "print(f\"Sensibilidad: {sensibilidad:.4f}\")\n",
    "print(f\"Especificidad: {especificidad:.4f}\")\n",
    "print(f\"G-mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b654581",
   "metadata": {},
   "source": [
    "### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit predictions\n",
    "\n",
    "test = pd.read_csv(\"/home/ander/Documentos/Universidad/ML/proyecto/test.csv\")\n",
    "test1 = test.drop(columns=[\"Surname\", \"id\", \"CustomerId\"])\n",
    "\n",
    "test_encoded = test1.copy()\n",
    "for column in categorical_columns:\n",
    "    one = encoders[column]\n",
    "    encoded = one.transform(test_encoded[[column]])\n",
    "    encoded_df = pd.DataFrame(encoded, columns=one.get_feature_names_out([column]), index=test_encoded.index)\n",
    "    test_encoded = pd.concat([test_encoded.drop(column, axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Gender column\n",
    "test_encoded['Gender'] = test_encoded['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Asegura columnas y orden\n",
    "test_encoded = test_encoded[X_train.columns]\n",
    "\n",
    "# Realiza las predicciones\n",
    "test_pred = best_brf.predict(test_encoded)\n",
    "test_pred_proba = best_brf.predict_proba(test_encoded)[:, 1]\n",
    "\n",
    "# Prepara el archivo de submit\n",
    "submit = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Exited\": test_pred_proba\n",
    "})\n",
    "\n",
    "# Guarda el archivo\n",
    "submit.to_csv(\"brf_250.csv\", index=False)\n",
    "print(\"Archivo de submit guardado como submission_brf_250.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_all_roc_with_xgb_variants(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'RandomForest': best_rf,\n",
    "        'GradientBoosting': pipeline_gb,\n",
    "        'XGBoost (Optuna)': best_xgb,\n",
    "        'LightGBM': pipeline_lgbm,\n",
    "        'LogisticRegression': pipeline_lr,\n",
    "        'ExtraTrees': best_et,\n",
    "        'EasyEnsemble': best_eec,\n",
    "        'RUSBoost': best_rus,\n",
    "        'BalancedBagging': best_bbc,\n",
    "        'BalancedRF': best_brf\n",
    "    }\n",
    "\n",
    "    # XGBoost variantes\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.1107,\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 10,\n",
    "        'subsample': 0.8266,\n",
    "        'colsample_bytree': 0.7236,\n",
    "        'gamma': 4.514,\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    # Sin sampler\n",
    "    xgb_no_sampler = XGBClassifier(**xgb_params)\n",
    "    xgb_no_sampler.fit(X_train, y_train)\n",
    "    y_pred_proba_xgb_no_sampler = xgb_no_sampler.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Con SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    xgb_smote = XGBClassifier(**xgb_params)\n",
    "    xgb_smote.fit(X_train_smote, y_train_smote)\n",
    "    y_pred_proba_xgb_smote = xgb_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Con ADASYN\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "    xgb_adasyn = XGBClassifier(**xgb_params)\n",
    "    xgb_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "    y_pred_proba_xgb_adasyn = xgb_adasyn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    # Otros modelos\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Si el modelo no está entrenado, entrenar (solo para modelos no pipeline)\n",
    "            if hasattr(model, 'fit') and not hasattr(model, 'classes_'):\n",
    "                model.fit(X_train, y_train)\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            elif hasattr(model, 'decision_function'):\n",
    "                from sklearn.preprocessing import MinMaxScaler\n",
    "                scores = model.decision_function(X_test).reshape(-1, 1)\n",
    "                y_pred_proba = MinMaxScaler().fit_transform(scores).flatten()\n",
    "            else:\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.5f})')\n",
    "        except Exception as e:\n",
    "            print(f'No se pudo graficar {name}: {e}')\n",
    "\n",
    "    # Agrega la curva ROC del modelo LightGBM(TPOT) desde CSV\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        roc_tpot = pd.read_csv('roc_modelo.csv')\n",
    "        fpr_tpot = roc_tpot['fpr']\n",
    "        tpr_tpot = roc_tpot['tpr']\n",
    "        auc_tpot = roc_tpot['auc'].iloc[0] if 'auc' in roc_tpot.columns else auc(fpr_tpot, tpr_tpot)\n",
    "        plt.plot(fpr_tpot, tpr_tpot, lw=2, color='orange', linestyle='-.', label=f'LightGBM(TPOT) (AUC = {auc_tpot:.5f})')\n",
    "    except Exception as e:\n",
    "        print(f'No se pudo cargar roc_modelo.csv: {e}')\n",
    "\n",
    "    # XGBoost variantes\n",
    "    for name, y_pred_proba in zip([\n",
    "        'XGBoost (N)',\n",
    "        'XGBoost (P - SMOTE)',\n",
    "        'XGBoost (P - ADASYN)'\n",
    "    ], [y_pred_proba_xgb_no_sampler, y_pred_proba_xgb_smote, y_pred_proba_xgb_adasyn]):\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, linestyle='--', label=f'{name} (AUC = {roc_auc:.5f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle=':')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves de los Modelos Optimizados y XGBoost variantes')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Llama la función para graficar todas las curvas ROC incluyendo XGBoost variantes\n",
    "plot_all_roc_with_xgb_variants(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
